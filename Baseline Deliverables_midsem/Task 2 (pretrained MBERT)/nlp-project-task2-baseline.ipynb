{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":11401154,"sourceType":"datasetVersion","datasetId":7140865},{"sourceId":11401174,"sourceType":"datasetVersion","datasetId":7140883},{"sourceId":11406905,"sourceType":"datasetVersion","datasetId":7145306},{"sourceId":11402619,"sourceType":"datasetVersion","datasetId":7141992}],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport re\nimport os\nfrom sklearn.model_selection import train_test_split\n\n# Paths to datasets\ndata_path = '/kaggle/input/train-dataset-trans-learn'\nfiles = ['train_en_l1.csv', 'train_hi_l1.csv', 'train_ta_l1.csv']\n\n# Function to load datasets with language info\ndef load_data(files, data_path):\n    dfs = []\n    for file in files:\n        lang = file.split('_')[1] \n        df = pd.read_csv(os.path.join(data_path, file))\n        df['language'] = lang\n        dfs.append(df)\n    return pd.concat(dfs, ignore_index=True)\n\n# Load all datasets\ndf = load_data(files, data_path)\n\n# Drop rows where label is still missing\ndf.dropna(subset=['label'], inplace=True)\n\n# Step 3: Enhanced Text Cleaning\ndef clean_text(text):\n    text = text.lower()  # Lowercase the text\n\n    # Preserve hashtags and mentions\n    text = re.sub(r'@\\w+', '[USER]', text)  # Replace handles with [USER]\n    text = re.sub(r'http\\S+|www\\S+', '[URL]', text)  # Replace URLs with [URL]\n\n    # Remove HTML tags\n    text = re.sub(r'<.*?>', ' ', text)\n\n    # Keep alphanumeric (English, Hindi, Tamil) + hashtags and mentions\n    text = re.sub(r'[^a-zA-Z0-9#@ऀ-ॿ஀-௿\\s]', ' ', text)\n\n    # Remove extra whitespace\n    text = re.sub(r'\\s+', ' ', text).strip()\n\n    return text\n\ndf['cleaned_text'] = df['text'].apply(clean_text)\n\n# Step 4: Finalize Dataset\ndf_final = df[['cleaned_text', 'label', 'language']]\n\n# Save cleaned dataset\ndf_final.to_csv('cleaned_l1.csv', index=False)\n\nprint(\"Data preparation complete! Cleaned dataset saved as 'cleaned_l1.csv'.\")\n\ntrain_df, val_df = train_test_split(df, test_size=0.2, random_state=42, stratify=df['label'])\n\nprint(f\"Training set: {len(train_df)} samples\")\nprint(f\"Validation set: {len(val_df)} samples\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-04-15T09:11:28.767158Z","iopub.execute_input":"2025-04-15T09:11:28.767716Z","iopub.status.idle":"2025-04-15T09:11:30.807380Z","shell.execute_reply.started":"2025-04-15T09:11:28.767693Z","shell.execute_reply":"2025-04-15T09:11:30.806728Z"}},"outputs":[{"name":"stdout","text":"Data preparation complete! Cleaned dataset saved as 'cleaned_l1.csv'.\nTraining set: 44946 samples\nValidation set: 11237 samples\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"import torch\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.optim import AdamW\nfrom transformers import BertTokenizer, BertModel\nfrom sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\nfrom torch import nn\nimport numpy as np\nfrom tqdm import tqdm\n\n# Set random seeds for reproducibility\ntorch.manual_seed(42)\nnp.random.seed(42)\n\n# Step 5: Load mBERT tokenizer and model\ntokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\nbert_model = BertModel.from_pretrained('bert-base-multilingual-cased')\n\n# Step 6: Create a custom dataset class\nclass AbusiveLanguageDataset(Dataset):\n    def __init__(self, texts, labels, tokenizer, max_len=128):\n        self.texts = texts\n        self.labels = labels\n        self.tokenizer = tokenizer\n        self.max_len = max_len\n    \n    def __len__(self):\n        return len(self.texts)\n    \n    def __getitem__(self, idx):\n        text = str(self.texts[idx])\n        label = self.labels[idx]\n        \n        # Tokenize the text\n        encoding = self.tokenizer(\n            text,\n            add_special_tokens=True,\n            max_length=self.max_len,\n            return_token_type_ids=True,\n            padding='max_length',\n            truncation=True,\n            return_attention_mask=True,\n            return_tensors='pt'\n        )\n        \n        return {\n            'input_ids': encoding['input_ids'].flatten(),\n            'attention_mask': encoding['attention_mask'].flatten(),\n            'token_type_ids': encoding['token_type_ids'].flatten(),\n            'label': torch.tensor(label, dtype=torch.long)\n        }\n\n# Step 7: Create the classifier model\nclass AbusiveLanguageClassifier(nn.Module):\n    def __init__(self, bert_model, num_classes=2, dropout_rate=0.3):\n        super(AbusiveLanguageClassifier, self).__init__()\n        self.bert = bert_model\n        self.dropout = nn.Dropout(dropout_rate)\n        self.linear = nn.Linear(768, num_classes)  # 768 is the size of BERT embeddings\n    \n    def forward(self, input_ids, attention_mask, token_type_ids):\n        # Get BERT outputs\n        outputs = self.bert(\n            input_ids=input_ids,\n            attention_mask=attention_mask,\n            token_type_ids=token_type_ids\n        )\n        \n        # Use the CLS token representation\n        pooled_output = outputs.pooler_output\n        \n        # Apply dropout and the classifier\n        dropout_output = self.dropout(pooled_output)\n        logits = self.linear(dropout_output)\n        \n        return logits\n\n# Step 8: Training function\ndef train_model(model, train_loader, val_loader, learning_rate=2e-5, epochs=3):\n    # Use GPU if available\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    print(f\"Using device: {device}\")\n    \n    model = model.to(device)\n    \n    # Set up optimizer\n    optimizer = AdamW(model.parameters(), lr=learning_rate)\n    \n    # Set up loss function\n    criterion = nn.CrossEntropyLoss()\n    \n    # Training loop\n    best_val_f1 = 0\n    for epoch in range(epochs):\n        print(f\"Epoch {epoch + 1}/{epochs}\")\n        \n        # Training phase\n        model.train()\n        train_loss = 0\n        \n        for batch in tqdm(train_loader, desc=\"Training\"):\n            # Move batch to device\n            input_ids = batch['input_ids'].to(device)\n            attention_mask = batch['attention_mask'].to(device)\n            token_type_ids = batch['token_type_ids'].to(device)\n            labels = batch['label'].to(device)\n            \n            # Forward pass\n            optimizer.zero_grad()\n            outputs = model(input_ids, attention_mask, token_type_ids)\n            \n            # Calculate loss\n            loss = criterion(outputs, labels)\n            train_loss += loss.item()\n            \n            # Backward pass and optimize\n            loss.backward()\n            optimizer.step()\n        \n        avg_train_loss = train_loss / len(train_loader)\n        print(f\"Average training loss: {avg_train_loss:.4f}\")\n        \n        # Validation phase\n        model.eval()\n        val_loss = 0\n        val_preds = []\n        val_true = []\n        \n        with torch.no_grad():\n            for batch in tqdm(val_loader, desc=\"Validation\"):\n                # Move batch to device\n                input_ids = batch['input_ids'].to(device)\n                attention_mask = batch['attention_mask'].to(device)\n                token_type_ids = batch['token_type_ids'].to(device)\n                labels = batch['label'].to(device)\n                \n                # Forward pass\n                outputs = model(input_ids, attention_mask, token_type_ids)\n                \n                # Calculate loss\n                loss = criterion(outputs, labels)\n                val_loss += loss.item()\n                \n                # Get predictions\n                _, preds = torch.max(outputs, dim=1)\n                val_preds.extend(preds.cpu().tolist())\n                val_true.extend(labels.cpu().tolist())\n        \n        # Calculate metrics\n        val_accuracy = accuracy_score(val_true, val_preds)\n        val_f1 = f1_score(val_true, val_preds, average='macro')\n        val_precision = precision_score(val_true, val_preds, average='macro')\n        val_recall = recall_score(val_true, val_preds, average='macro')\n        \n        avg_val_loss = val_loss / len(val_loader)\n        print(f\"Validation Loss: {avg_val_loss:.4f}\")\n        print(f\"Validation Metrics - Accuracy: {val_accuracy:.4f}, Macro F1: {val_f1:.4f}, Precision: {val_precision:.4f}, Recall: {val_recall:.4f}\")\n        \n        # Save the best model\n        if val_f1 > best_val_f1:\n            best_val_f1 = val_f1\n            torch.save(model.state_dict(), 'best_mbert_abusive_classifier.pt')\n            print(\"Saved best model!\")\n    \n    return model\n\n# Step 9: Prepare data loaders\n# Convert labels to numeric if they're strings\nlabel_to_id = {'NOT': 0, 'HOF': 1} if isinstance(train_df['label'].iloc[0], str) else None\n\nif label_to_id:\n    train_labels = [label_to_id[label] for label in train_df['label']]\n    val_labels = [label_to_id[label] for label in val_df['label']]\nelse:\n    train_labels = train_df['label'].tolist()\n    val_labels = val_df['label'].tolist()\n\n# Create datasets\ntrain_dataset = AbusiveLanguageDataset(\n    texts=train_df['cleaned_text'].tolist(),\n    labels=train_labels,\n    tokenizer=tokenizer\n)\n\nval_dataset = AbusiveLanguageDataset(\n    texts=val_df['cleaned_text'].tolist(),\n    labels=val_labels,\n    tokenizer=tokenizer\n)\n\n# Create data loaders\nbatch_size = 16\ntrain_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=batch_size)\n\n# Step 10: Initialize model\nclassifier = AbusiveLanguageClassifier(bert_model)\n\n# Step 11: Train model\ntrained_model = train_model(\n    model=classifier,\n    train_loader=train_loader,\n    val_loader=val_loader,\n    learning_rate=2e-5,\n    epochs=3\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T09:11:30.808740Z","iopub.execute_input":"2025-04-15T09:11:30.808997Z","iopub.status.idle":"2025-04-15T09:43:52.857037Z","shell.execute_reply.started":"2025-04-15T09:11:30.808979Z","shell.execute_reply":"2025-04-15T09:43:52.856155Z"}},"outputs":[{"name":"stderr","text":"2025-04-15 09:11:43.199977: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1744708303.410514      31 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1744708303.463397      31 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/49.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0b27648ee1984cf48d75d5143e19745a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/996k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f9af6b2ecd8b42538925972e5bf1262d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.96M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5a9a21adc9054535a70bc474e8f53780"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/625 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0a35399269ce451bba9cb7c5af442be7"}},"metadata":{}},{"name":"stderr","text":"Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/714M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cad4eb212f944bb99aca94cbb4201271"}},"metadata":{}},{"name":"stdout","text":"Using device: cuda\nEpoch 1/3\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 2810/2810 [09:53<00:00,  4.73it/s]\n","output_type":"stream"},{"name":"stdout","text":"Average training loss: 0.3422\n","output_type":"stream"},{"name":"stderr","text":"Validation: 100%|██████████| 703/703 [00:42<00:00, 16.42it/s]\n","output_type":"stream"},{"name":"stdout","text":"Validation Loss: 0.2990\nValidation Metrics - Accuracy: 0.8782, Macro F1: 0.8734, Precision: 0.8721, Recall: 0.8749\nSaved best model!\nEpoch 2/3\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 2810/2810 [09:53<00:00,  4.74it/s]\n","output_type":"stream"},{"name":"stdout","text":"Average training loss: 0.2509\n","output_type":"stream"},{"name":"stderr","text":"Validation: 100%|██████████| 703/703 [00:42<00:00, 16.40it/s]\n","output_type":"stream"},{"name":"stdout","text":"Validation Loss: 0.2669\nValidation Metrics - Accuracy: 0.8867, Macro F1: 0.8825, Precision: 0.8806, Recall: 0.8846\nSaved best model!\nEpoch 3/3\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 2810/2810 [09:53<00:00,  4.74it/s]\n","output_type":"stream"},{"name":"stdout","text":"Average training loss: 0.2020\n","output_type":"stream"},{"name":"stderr","text":"Validation: 100%|██████████| 703/703 [00:42<00:00, 16.40it/s]\n","output_type":"stream"},{"name":"stdout","text":"Validation Loss: 0.2844\nValidation Metrics - Accuracy: 0.8872, Macro F1: 0.8828, Precision: 0.8817, Recall: 0.8840\nSaved best model!\n","output_type":"stream"}],"execution_count":3},{"cell_type":"markdown","source":"Preparation of test data\n","metadata":{}},{"cell_type":"code","source":"# Paths to datasets\ndata_path = '/kaggle/input/test-dataset-trans-learn'\nfiles = ['test_en_l1.csv', 'test_hi_l1.csv', 'test_ta_l1.csv']\n\n# Function to load datasets with language info\ndef load_data(files, data_path):\n    dfs = []\n    for file in files:\n        lang = file.split('_')[1] \n        df = pd.read_csv(os.path.join(data_path, file))\n        df['language'] = lang\n        dfs.append(df)\n    return pd.concat(dfs, ignore_index=True)\n\n# Load all datasets\ndf = load_data(files, data_path)\n\n# Drop rows where label is still missing\ndf.dropna(subset=['label'], inplace=True)\n\n# Step 3: Enhanced Text Cleaning\ndef clean_text(text):\n    text = text.lower()  # Lowercase the text\n\n    # Preserve hashtags and mentions\n    text = re.sub(r'@\\w+', '[USER]', text)  # Replace handles with [USER]\n    text = re.sub(r'http\\S+|www\\S+', '[URL]', text)  # Replace URLs with [URL]\n\n    # Remove HTML tags\n    text = re.sub(r'<.*?>', ' ', text)\n\n    # Keep alphanumeric (English, Hindi, Tamil) + hashtags and mentions\n    text = re.sub(r'[^a-zA-Z0-9#@ऀ-ॿ஀-௿\\s]', ' ', text)\n\n    # Remove extra whitespace\n    text = re.sub(r'\\s+', ' ', text).strip()\n\n    return text\n\ndf['cleaned_text'] = df['text'].apply(clean_text)\n\n# Step 4: Finalize Dataset\ndf_final = df[['cleaned_text', 'label', 'language']]\n\n# Save cleaned dataset\ndf_final.to_csv('cleaned_test.csv', index=False)\n\nprint(\"Data preparation complete! Cleaned dataset saved as 'cleaned_test.csv'.\")\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T09:43:52.858257Z","iopub.execute_input":"2025-04-15T09:43:52.858844Z","iopub.status.idle":"2025-04-15T09:43:53.277413Z","shell.execute_reply.started":"2025-04-15T09:43:52.858819Z","shell.execute_reply":"2025-04-15T09:43:53.276594Z"}},"outputs":[{"name":"stdout","text":"Data preparation complete! Cleaned dataset saved as 'cleaned_test.csv'.\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"import pandas as pd\nimport torch\nfrom torch.utils.data import DataLoader\nfrom transformers import BertTokenizer, BertModel\nfrom sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report, confusion_matrix\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom tqdm import tqdm\n\n# Load the test data\ntest_df = pd.read_csv('/kaggle/working/cleaned_test.csv')\nprint(f\"Test dataset shape: {test_df.shape}\")\nprint(test_df['label'].value_counts())\n\n# Create DataLoader for test data\nbatch_size = 16\n\nlabel_to_id = {'NOT': 0, 'HOF': 1} if isinstance(test_df['label'].iloc[0], str) else None\n\nif label_to_id:\n    test_labels = [label_to_id[label] for label in test_df['label']]\nelse:\n    test_labels = test_df['label'].tolist()\n    \ntest_dataset = AbusiveLanguageDataset(texts=test_df['cleaned_text'].tolist(),\n    labels=test_labels,\n    tokenizer=tokenizer\n)\n\ntest_loader = DataLoader(test_dataset, batch_size=batch_size)\n\n# Set device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# Load your trained model\nbert_model = BertModel.from_pretrained('bert-base-multilingual-cased')\nclassifier = AbusiveLanguageClassifier(bert_model)  # Using your existing class\nclassifier.load_state_dict(torch.load('best_mbert_abusive_classifier.pt', map_location=device))\nclassifier.to(device)\nclassifier.eval()\n\n# Evaluate on test set\ntest_preds = []\ntest_true = []\ntest_probs = []\n\nwith torch.no_grad():\n    for batch in tqdm(test_loader, desc=\"Testing\"):\n        input_ids = batch['input_ids'].to(device)\n        attention_mask = batch['attention_mask'].to(device)\n        token_type_ids = batch['token_type_ids'].to(device)\n        labels = batch['label'].to(device)\n        \n        outputs = classifier(input_ids, attention_mask, token_type_ids)\n        probabilities = torch.softmax(outputs, dim=1)\n        _, preds = torch.max(outputs, dim=1)\n        \n        test_preds.extend(preds.cpu().tolist())\n        test_true.extend(labels.cpu().tolist())\n        test_probs.extend(probabilities[:, 1].cpu().tolist())  # Probability of positive class\n\n# Calculate metrics\ntest_accuracy = accuracy_score(test_true, test_preds)\ntest_f1_macro = f1_score(test_true, test_preds, average='macro')\ntest_f1_weighted = f1_score(test_true, test_preds, average='weighted')\ntest_precision = precision_score(test_true, test_preds, average='macro')\ntest_recall = recall_score(test_true, test_preds, average='macro')\n\n# Print metrics\nprint(\"\\nTest Evaluation Results:\")\nprint(f\"Accuracy: {test_accuracy:.4f}\")\nprint(f\"F1 Score (Macro): {test_f1_macro:.4f}\")\nprint(f\"F1 Score (Weighted): {test_f1_weighted:.4f}\")\nprint(f\"Precision: {test_precision:.4f}\")\nprint(f\"Recall: {test_recall:.4f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T09:43:53.279237Z","iopub.execute_input":"2025-04-15T09:43:53.279454Z","iopub.status.idle":"2025-04-15T09:45:23.048477Z","shell.execute_reply.started":"2025-04-15T09:43:53.279438Z","shell.execute_reply":"2025-04-15T09:45:23.047623Z"}},"outputs":[{"name":"stdout","text":"Test dataset shape: (19511, 3)\nlabel\n1    11943\n0     7568\nName: count, dtype: int64\nUsing device: cuda\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_31/1581954089.py:40: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  classifier.load_state_dict(torch.load('best_mbert_abusive_classifier.pt', map_location=device))\nTesting: 100%|██████████| 1220/1220 [01:14<00:00, 16.43it/s]","output_type":"stream"},{"name":"stdout","text":"\nTest Evaluation Results:\nAccuracy: 0.8921\nF1 Score (Macro): 0.8865\nF1 Score (Weighted): 0.8921\nPrecision: 0.8861\nRecall: 0.8869\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":5},{"cell_type":"markdown","source":"Fine Tuning","metadata":{}},{"cell_type":"code","source":"tuned_train_df = pd.read_csv(\"/kaggle/input/transfer-learn-test/cleaned_dataset_l1.csv\")\n\ntext_column = 'cleaned_text'\nlabel_column = 'label'\n\ntuned_train_df, tuned_val_df = train_test_split(\n    tuned_train_df,\n    test_size=0.2,\n    random_state=42,\n    stratify=tuned_train_df[label_column]\n)\n\ntokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n\n# Check if labels are strings or integers\nif isinstance(train_df[label_column].iloc[0], str):\n    # Adjust this mapping based on your dataset\n    label_to_id = {'NOT': 0, 'HOF': 1}\n    train_labels = [label_to_id[label] for label in train_df[label_column]]\n    val_labels = [label_to_id[label] for label in val_df[label_column]]\nelse:\n    train_labels = train_df[label_column].tolist()\n    val_labels = val_df[label_column].tolist()\n\n# Create datasets\ntrain_dataset = AbusiveLanguageDataset(\n    texts=train_df[text_column].tolist(),\n    labels=train_labels,\n    tokenizer=tokenizer\n)\n\nval_dataset = AbusiveLanguageDataset(\n    texts=val_df[text_column].tolist(),\n    labels=val_labels,\n    tokenizer=tokenizer\n)\n\n# Create data loaders\nbatch_size = 16\ntrain_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=batch_size)\n\n# Load pre-trained model\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# Load base BERT model\nbert_model = BertModel.from_pretrained('bert-base-multilingual-cased')\nbert_model.to(device)\n\n# Initialize classifier\nclassifier = AbusiveLanguageClassifier(bert_model)\n\n# Load your previously trained model weights\nclassifier.load_state_dict(torch.load('best_mbert_abusive_classifier.pt', map_location=device))\nclassifier.to(device)\n\n# _________________________________________\n#   FINE TUNING FUNCTION\n# ________________________________\n\ndef fine_tune_model(model, train_loader, val_loader, learning_rate=2e-5, epochs=3):\n    model.train()\n    \n    # We'll use a smaller learning rate for fine-tuning\n    optimizer = AdamW(model.parameters(), lr=learning_rate)\n    criterion = torch.nn.CrossEntropyLoss()\n    \n    best_val_f1 = 0\n    \n    for epoch in range(epochs):\n        print(f\"Epoch {epoch + 1}/{epochs}\")\n        \n        # Training phase\n        model.train()\n        train_loss = 0\n        \n        for batch in tqdm(train_loader, desc=\"Training\"):\n            input_ids = batch['input_ids'].to(device)\n            attention_mask = batch['attention_mask'].to(device)\n            token_type_ids = batch['token_type_ids'].to(device)\n            labels = batch['label'].to(device)\n            \n            optimizer.zero_grad()\n            outputs = model(input_ids, attention_mask, token_type_ids)\n            \n            loss = criterion(outputs, labels)\n            train_loss += loss.item()\n            \n            loss.backward()\n            optimizer.step()\n        \n        avg_train_loss = train_loss / len(train_loader)\n        print(f\"Average training loss: {avg_train_loss:.4f}\")\n        \n        # Validation phase\n        model.eval()\n        val_loss = 0\n        val_preds = []\n        val_true = []\n        \n        with torch.no_grad():\n            for batch in tqdm(val_loader, desc=\"Validation\"):\n                input_ids = batch['input_ids'].to(device)\n                attention_mask = batch['attention_mask'].to(device)\n                token_type_ids = batch['token_type_ids'].to(device)\n                labels = batch['label'].to(device)\n                \n                outputs = model(input_ids, attention_mask, token_type_ids)\n                \n                loss = criterion(outputs, labels)\n                val_loss += loss.item()\n                \n                _, preds = torch.max(outputs, dim=1)\n                val_preds.extend(preds.cpu().tolist())\n                val_true.extend(labels.cpu().tolist())\n        \n        # Calculate metrics\n        val_accuracy = accuracy_score(val_true, val_preds)\n        val_f1 = f1_score(val_true, val_preds, average='macro')\n        val_precision = precision_score(val_true, val_preds, average='macro')\n        val_recall = recall_score(val_true, val_preds, average='macro')\n        \n        avg_val_loss = val_loss / len(val_loader)\n        print(f\"Validation Loss: {avg_val_loss:.4f}\")\n        print(f\"Validation Metrics - Accuracy: {val_accuracy:.4f}, F1: {val_f1:.4f}, Precision: {val_precision:.4f}, Recall: {val_recall:.4f}\")\n        \n        # Save the best model\n        if val_f1 > best_val_f1:\n            best_val_f1 = val_f1\n            torch.save(model.state_dict(), 'best_fine_tuned_classifier.pt')\n            print(\"Saved best fine-tuned model!\")\n    \n    return model\n\n#  FINE TUNE THE MODEL\n\nfine_tuned_model = fine_tune_model(\n    model=classifier,\n    train_loader=train_loader,\n    val_loader=val_loader,\n    learning_rate=5e-6,  # Lower learning rate for fine-tuning\n    epochs=3\n)\n\n# Load best fine-tuned model for evaluation\nclassifier.load_state_dict(torch.load('best_fine_tuned_classifier.pt'))\nclassifier.eval()\n\n# Evaluate on validation set\nval_preds = []\nval_true = []\n\nwith torch.no_grad():\n    for batch in tqdm(val_loader, desc=\"Final Evaluation\"):\n        input_ids = batch['input_ids'].to(device)\n        attention_mask = batch['attention_mask'].to(device)\n        token_type_ids = batch['token_type_ids'].to(device)\n        labels = batch['label'].to(device)\n        \n        outputs = classifier(input_ids, attention_mask, token_type_ids)\n        _, preds = torch.max(outputs, dim=1)\n        \n        val_preds.extend(preds.cpu().tolist())\n        val_true.extend(labels.cpu().tolist())\n\n# Calculate final metrics\nfinal_accuracy = accuracy_score(val_true, val_preds)\nfinal_f1 = f1_score(val_true, val_preds, average='macro')\nfinal_precision = precision_score(val_true, val_preds, average='macro')\nfinal_recall = recall_score(val_true, val_preds, average='macro')\n\nprint(\"\\nFinal Evaluation Results:\")\nprint(f\"Accuracy: {final_accuracy:.4f}\")\nprint(f\"F1 Score: {final_f1:.4f}\")\nprint(f\"Precision: {final_precision:.4f}\")\nprint(f\"Recall: {final_recall:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T09:45:23.049399Z","iopub.execute_input":"2025-04-15T09:45:23.050156Z","iopub.status.idle":"2025-04-15T10:18:09.796339Z","shell.execute_reply.started":"2025-04-15T09:45:23.050130Z","shell.execute_reply":"2025-04-15T10:18:09.795636Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_31/745888688.py:55: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  classifier.load_state_dict(torch.load('best_mbert_abusive_classifier.pt', map_location=device))\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/3\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 2810/2810 [09:53<00:00,  4.73it/s]\n","output_type":"stream"},{"name":"stdout","text":"Average training loss: 0.1215\n","output_type":"stream"},{"name":"stderr","text":"Validation: 100%|██████████| 703/703 [00:42<00:00, 16.40it/s]\n","output_type":"stream"},{"name":"stdout","text":"Validation Loss: 0.3231\nValidation Metrics - Accuracy: 0.8883, F1: 0.8842, Precision: 0.8821, Recall: 0.8868\nSaved best fine-tuned model!\nEpoch 2/3\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 2810/2810 [09:53<00:00,  4.73it/s]\n","output_type":"stream"},{"name":"stdout","text":"Average training loss: 0.0881\n","output_type":"stream"},{"name":"stderr","text":"Validation: 100%|██████████| 703/703 [00:42<00:00, 16.36it/s]\n","output_type":"stream"},{"name":"stdout","text":"Validation Loss: 0.3365\nValidation Metrics - Accuracy: 0.8858, F1: 0.8819, Precision: 0.8793, Recall: 0.8853\nEpoch 3/3\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 2810/2810 [09:53<00:00,  4.73it/s]\n","output_type":"stream"},{"name":"stdout","text":"Average training loss: 0.0674\n","output_type":"stream"},{"name":"stderr","text":"Validation: 100%|██████████| 703/703 [00:42<00:00, 16.40it/s]\n/tmp/ipykernel_31/745888688.py:147: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  classifier.load_state_dict(torch.load('best_fine_tuned_classifier.pt'))\n","output_type":"stream"},{"name":"stdout","text":"Validation Loss: 0.4093\nValidation Metrics - Accuracy: 0.8841, F1: 0.8796, Precision: 0.8782, Recall: 0.8812\n","output_type":"stream"},{"name":"stderr","text":"Final Evaluation: 100%|██████████| 703/703 [00:42<00:00, 16.43it/s]","output_type":"stream"},{"name":"stdout","text":"\nFinal Evaluation Results:\nAccuracy: 0.8883\nF1 Score: 0.8842\nPrecision: 0.8821\nRecall: 0.8868\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"# Paths to datasets\ndata_path = '/kaggle/input/fine-tuned-test-dataset'\nfiles = ['test_en_l1.csv', 'test_hi_l1.csv','test_ta_l1.csv']\n\n# Function to load datasets with language info\ndef load_data(files, data_path):\n    dfs = []\n    for file in files:\n        lang = file.split('_')[1]\n        file_path = os.path.join(data_path, file)\n        try:\n            # Try with 'c' engine (faster)\n            # \"engine\": \"python\", \"on_bad_lines\": \"warn\"\n            df = pd.read_csv(file_path, engine='python', encoding='utf-8', on_bad_lines='warn')\n        except Exception as e:\n            print(f\"Failed with 'c' engine for {file}: {e}, trying 'python' engine...\")\n            try:\n                df = pd.read_csv(file_path, engine='python', encoding='utf-8', error_bad_lines=False)\n            except Exception as e2:\n                print(f\"Skipping {file} due to read error: {e2}\")\n                continue\n\n        df['language'] = lang\n        dfs.append(df)\n    return pd.concat(dfs, ignore_index=True)\n\n# Load all datasets\ndf = load_data(files, data_path)\n\n# Step 1: Handle Missing Values\ndef clean_missing_values(df):\n    # Replace 'NL' with np.nan for easier handling\n    df.replace('NL', np.nan, inplace=True)\n\n    # Drop rows where all annotations are missing\n    annotator_cols = [col for col in df.columns if re.match(r\".*a[1-6]\", col)]\n    df.dropna(subset=annotator_cols, how='all', inplace=True)\n\n    return df, annotator_cols\n\ndf, annotator_cols = clean_missing_values(df)\n\n# Step 2: Create Final Label (Majority Vote)\ndef majority_vote(row):\n    votes = row[annotator_cols].dropna().values.astype(float)\n    if len(votes) == 0:\n        return np.nan\n    return 1.0 if votes.mean() >= 0.5 else 0.0\n\ndf['label'] = df.apply(majority_vote, axis=1)\n\n# Drop rows where label is still missing\ndf.dropna(subset=['label'], inplace=True)\n\n# Step 3: Enhanced Text Cleaning\ndef clean_text(text):\n    text = text.lower()  # Lowercase the text\n\n    # Preserve hashtags and mentions\n    text = re.sub(r'@\\w+', '[USER]', text)  # Replace handles with [USER]\n    text = re.sub(r'http\\S+|www\\S+', '[URL]', text)  # Replace URLs with [URL]\n\n    # Remove HTML tags\n    text = re.sub(r'<.*?>', ' ', text)\n\n    # Keep alphanumeric (English, Hindi, Tamil) + hashtags and mentions\n    text = re.sub(r'[^a-zA-Z0-9#@ऀ-ॿ஀-௿\\s]', ' ', text)\n\n    # Remove extra whitespace\n    text = re.sub(r'\\s+', ' ', text).strip()\n\n    return text\n\ndf['cleaned_text'] = df['text'].apply(clean_text)\n\n# Step 4: Finalize Dataset\ndf_final = df[['cleaned_text', 'label', 'language']]\n\n# Save cleaned dataset\ndf_final.to_csv('fine_tuned_test.csv', index=False)\n\nprint(\"Data preparation complete! Cleaned dataset saved as 'fine_tuned_test.csv'.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T10:18:09.797118Z","iopub.execute_input":"2025-04-15T10:18:09.797403Z","iopub.status.idle":"2025-04-15T10:18:11.310395Z","shell.execute_reply.started":"2025-04-15T10:18:09.797378Z","shell.execute_reply":"2025-04-15T10:18:11.309784Z"}},"outputs":[{"name":"stdout","text":"Data preparation complete! Cleaned dataset saved as 'fine_tuned_test.csv'.\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"fine_tuned_test_df = pd.read_csv('fine_tuned_test.csv')\n\ntext_column = 'cleaned_text'\nlabel_column = 'label'\n\ntokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n\nif isinstance(fine_tuned_test_df[label_column].iloc[0], str):\n    label_to_id = {'NOT': 0, 'HOF': 1}  # Adjust based on your labels\n    test_labels = [label_to_id[label] for label in fine_tuned_test_df[label_column]]\nelse:\n    test_labels = fine_tuned_test_df[label_column].tolist()\n\n# Create test dataset\ntest_dataset = AbusiveLanguageDataset(\n    texts=fine_tuned_test_df[text_column].tolist(),\n    labels=test_labels,\n    tokenizer=tokenizer\n)\n\n# Create test dataloader\nbatch_size = 16\ntest_loader = DataLoader(test_dataset, batch_size=batch_size)\n\n# Set device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# Load BERT model\nbert_model = BertModel.from_pretrained('bert-base-multilingual-cased')\nbert_model.to(device)\n\n# Initialize classifier with the fine-tuned model\nclassifier = AbusiveLanguageClassifier(bert_model)\nclassifier.load_state_dict(torch.load('best_fine_tuned_classifier.pt', map_location=device))\nclassifier.to(device)\nclassifier.eval()\n\n# Evaluate on test set\ntest_preds = []\ntest_true = []\ntest_probs = []\n\nwith torch.no_grad():\n    for batch in tqdm(test_loader, desc=\"Testing Fine-tuned Model\"):\n        input_ids = batch['input_ids'].to(device)\n        attention_mask = batch['attention_mask'].to(device)\n        token_type_ids = batch['token_type_ids'].to(device)\n        labels = batch['label'].to(device)\n        \n        outputs = classifier(input_ids, attention_mask, token_type_ids)\n        probabilities = torch.softmax(outputs, dim=1)\n        _, preds = torch.max(outputs, dim=1)\n        \n        test_preds.extend(preds.cpu().tolist())\n        test_true.extend(labels.cpu().tolist())\n        test_probs.extend(probabilities[:, 1].cpu().tolist())  # Probability of positive class\n\n# Calculate metrics\ntest_accuracy = accuracy_score(test_true, test_preds)\ntest_f1_macro = f1_score(test_true, test_preds, average='macro')\ntest_f1_weighted = f1_score(test_true, test_preds, average='weighted')\ntest_precision = precision_score(test_true, test_preds, average='macro')\ntest_recall = recall_score(test_true, test_preds, average='macro')\n\n# Print metrics\nprint(\"\\nFine-tuned Model Test Results:\")\nprint(f\"Accuracy: {test_accuracy:.4f}\")\nprint(f\"F1 Score (Macro): {test_f1_macro:.4f}\")\nprint(f\"F1 Score (Weighted): {test_f1_weighted:.4f}\")\nprint(f\"Precision: {test_precision:.4f}\")\nprint(f\"Recall: {test_recall:.4f}\")\n\n# Generate detailed classification report\nid_to_label = {0: 'NOT', 1: 'HOF'}  # Adjust based on your labels\ntarget_names = [id_to_label[i] for i in sorted(id_to_label.keys())]\nprint(\"\\nClassification Report:\")\nprint(classification_report(test_true, test_preds, target_names=target_names))\n\n# Generate confusion matrix\ncm = confusion_matrix(test_true, test_preds)\nplt.figure(figsize=(8, 6))\nsns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=target_names, yticklabels=target_names)\nplt.xlabel('Predicted')\nplt.ylabel('True')\nplt.title('Fine-tuned Model Confusion Matrix')\nplt.savefig('fine_tuned_confusion_matrix.png')\nplt.close()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T10:22:25.435456Z","iopub.execute_input":"2025-04-15T10:22:25.435696Z","iopub.status.idle":"2025-04-15T10:23:11.158531Z","shell.execute_reply.started":"2025-04-15T10:22:25.435680Z","shell.execute_reply":"2025-04-15T10:23:11.157776Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_31/4009473042.py:35: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  classifier.load_state_dict(torch.load('best_fine_tuned_classifier.pt', map_location=device))\nTesting Fine-tuned Model: 100%|██████████| 235/235 [00:14<00:00, 15.93it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nFine-tuned Model Test Results:\nAccuracy: 0.6743\nF1 Score (Macro): 0.6560\nF1 Score (Weighted): 0.6878\nPrecision: 0.6628\nRecall: 0.6931\n\nClassification Report:\n              precision    recall  f1-score   support\n\n         NOT       0.85      0.65      0.74      2631\n         HOF       0.47      0.74      0.58      1127\n\n    accuracy                           0.67      3758\n   macro avg       0.66      0.69      0.66      3758\nweighted avg       0.74      0.67      0.69      3758\n\n","output_type":"stream"}],"execution_count":9}]}